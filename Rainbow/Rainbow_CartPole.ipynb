{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Rainbow DQN PyTorch Implemetation\n",
    "=====================================\n",
    "**Author**: `Andres Quintela`\n",
    "\n",
    "Based on the PyTorch DQN tutorial\n",
    "\n",
    "Arcade Learning Environment on Cartpole\n",
    "\n",
    "November 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expererience replay\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    #saving a transition tuple\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    #sample a random number according to batch size\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,input_size,action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(input_size,16)\n",
    "        self.hidden2 = nn.Linear(16,16)\n",
    "        self.output = nn.Linear(16,action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.output(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "\n",
    "\n",
    "input_size=len(env.reset())\n",
    "action_size=env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "policy_net = DQN(input_size,action_size).to(device)\n",
    "target_net = DQN(input_size,action_size).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(),lr=0.01)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    \n",
    "    # Double DQN part\n",
    "    # Compute the action that would be taken according to the policy net on the next state\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    next_state_actions = policy_net(non_final_next_states).max(1)[1].view(len(non_final_next_states),1).detach()\n",
    "    \n",
    "    next_state_values_temp = target_net(non_final_next_states).gather(1, next_state_actions).detach()\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    next_state_values[non_final_mask] =next_state_values_temp.view(1,len(non_final_next_states))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    expected_state_action_values = expected_state_action_values.view(BATCH_SIZE,1)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n step return memory\n",
    "\n",
    "N_STEP=3\n",
    "N_STEP_GAMMA=0.9\n",
    "\n",
    "class Dynamic_memory(object):\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        self.size=size\n",
    "        self.memory=[0]*size\n",
    "        self.R=0\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self,arg):\n",
    "        if self.size > len(self.memory):\n",
    "            self.memory[self.position] = Transition(*args)\n",
    "            self.position = (self.position + 1)\n",
    "        else:\n",
    "            #append last reward and remove first one so that memory size is always equal to N_STEP\n",
    "            self.memory.append(arg)\n",
    "            self.memory.pop(0)\n",
    "    def pull_reward(self):\n",
    "        for i in range(self.size):\n",
    "            self.R += self.memory[self.size-1]*N_STEP_GAMMA**i\n",
    "        return self.R\n",
    "    def pull(self):\n",
    "        print(self.memory[0])\n",
    "        return self.memory[0]\n",
    "    def pull_n(self):\n",
    "        return self.memory[self.size-1]\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Loop\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "max_t=200  #maximum timesteps per episode\n",
    "\n",
    "episode_reward=[0]*num_episodes\n",
    "i_episode_reward=0\n",
    "\n",
    "steps_done=0\n",
    "\n",
    "\n",
    "reward_memory=Dynamic_memory(N_STEP)\n",
    "dynamic_memory_full=False\n",
    "state_memory=Dynamic_memory(N_STEP)\n",
    "action_memory=Dynamic_memory(N_STEP)\n",
    "next_state_memory=Dynamic_memory(N_STEP)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state=env.reset()\n",
    "    state=torch.tensor([state],dtype=torch.float,device=device)\n",
    "    print('episode',i_episode)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        if N_STEP > steps_done:\n",
    "            action = select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            next_state=torch.tensor([next_state],dtype=torch.float,device=device)\n",
    "            \n",
    "            if done:\n",
    "                next_state=None\n",
    "\n",
    "           # save transitions in dynamic lists\n",
    "            reward_memory.push(reward)\n",
    "            state_memory.push(state)\n",
    "            action_memory.push(action)\n",
    "            next_state_memory.push(next_state)\n",
    "            \n",
    "            # Move to the next state\n",
    "            state=next_state\n",
    "            \n",
    "            #accumulated reward for each episode\n",
    "            i_episode_reward += reward.item()\n",
    "            steps_done += 1 \n",
    "            \n",
    "            if done or (t>max_t):\n",
    "            #save episode reward\n",
    "                episode_reward[i_episode]=i_episode_reward\n",
    "                i_episode_reward=0\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            action = select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            next_state=torch.tensor([next_state],dtype=torch.float,device=device)\n",
    "            \n",
    "            if done:\n",
    "                next_state=None\n",
    "            \n",
    "            # save transitions in dynamic lists\n",
    "            reward_memory.push(reward)\n",
    "            state_memory.push(state)\n",
    "            action_memory.push(action)\n",
    "            next_state_memory.push(next_state)\n",
    "            \n",
    "            \n",
    "            #Store the transition in memory (state(t),action(t),next_state(t+n_step),sum(R)) with n-step modifs\n",
    "            memory.push(state_memory.pull(),action_memory.pull(), next_state_memory.pull_n(), reward_memory.pull_reward())\n",
    "\n",
    "           # Move to the next state\n",
    "            state=next_state\n",
    "            \n",
    "            #accumulated reward for each episode\n",
    "            i_episode_reward += reward.item()\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            optimize_model()\n",
    "            steps_done += 1 \n",
    "\n",
    "            \n",
    "            if done or (t>max_t):\n",
    "            #save episode reward\n",
    "                episode_reward[i_episode]=i_episode_reward\n",
    "                i_episode_reward=0\n",
    "                break\n",
    "        \n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(episode_reward[:i_episode])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

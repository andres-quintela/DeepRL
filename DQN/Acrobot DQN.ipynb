{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DQN PyTorch Implemetation\n",
    "=====================================\n",
    "**Author**: `Andres Quintela`\n",
    "\n",
    "Based on the PyTorch DQN tutorial\n",
    "\n",
    "Arcade Learning Environment on Cartpole\n",
    "\n",
    "November 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('Acrobot-v1').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    #saving a transition tuple\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    #sample a random number according to batch size\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "-------------\n",
    "\n",
    "3 convolutional layers and final linear fully conected layer.  \n",
    "Batch normalization is applied after every layer to eliminate outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.head = nn.Linear(784, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image extraction\n",
    "-------------\n",
    "\n",
    "The code below are utilities for extracting and processing rendered\n",
    "images from the environment. It uses the ``torchvision`` package, which\n",
    "makes it easy to compose image transforms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncvjNMvX/QQFezQqPbXwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))  # transpose into torch order (CH\n",
    "\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TARGET_UPDATE = 5\n",
    "LEARNING_RATE=0.01\n",
    "\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.SGD(policy_net.parameters(),lr=LEARNING_RATE)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(3)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Loop\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing episode:  0\n",
      "-429.54366870224476\n",
      "Executing episode:  1\n",
      "-478.45669239759445\n",
      "Executing episode:  2\n",
      "-488.48408156633377\n",
      "Executing episode:  3\n",
      "-291.8126023914665\n",
      "Executing episode:  4\n",
      "-264.5684394496493\n",
      "Executing episode:  5\n",
      "-284.60021804319695\n",
      "Executing episode:  6\n",
      "-268.41757171368226\n",
      "Executing episode:  7\n",
      "-359.4679203601554\n",
      "Executing episode:  8\n",
      "-329.72083564603236\n",
      "Executing episode:  9\n",
      "-163.4120639191242\n",
      "Executing episode:  10\n",
      "-184.62669610138983\n",
      "Executing episode:  11\n",
      "-172.98343811184168\n",
      "Executing episode:  12\n",
      "-147.3568568569608\n",
      "Executing episode:  13\n",
      "-339.3905631955713\n",
      "Executing episode:  14\n",
      "-134.85073013301007\n",
      "Executing episode:  15\n",
      "-114.45129557931796\n",
      "Executing episode:  16\n",
      "-129.2518223260995\n",
      "Executing episode:  17\n",
      "-190.77386532863602\n",
      "Executing episode:  18\n",
      "-301.06295616040006\n",
      "Executing episode:  19\n",
      "-460.6253538429737\n",
      "Executing episode:  20\n",
      "-197.42213053815067\n",
      "Executing episode:  21\n",
      "-205.11087225563824\n",
      "Executing episode:  22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a20fe3e8539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Select and perform an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mstate_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_numeric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a16404218a91>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0meps_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9397f1db5980>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "max_t=500  #maximum timesteps per episode\n",
    "\n",
    "episode_reward=[0]*num_episodes\n",
    "i_episode_reward=0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    print('Executing episode: ',i_episode)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        state_numeric, reward, done, _ = env.step(action.item())\n",
    "        if (state_numeric[0]< 0):\n",
    "            reward = torch.tensor([reward+1-state_numeric[0]], device=device)\n",
    "        elif (state_numeric[0]>0):\n",
    "            reward = torch.tensor([reward+(1-state_numeric[0])], device=device)\n",
    "        else:\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        \n",
    "        #accumulated reward for each episode\n",
    "        i_episode_reward += reward.item()\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done or (t>max_t):\n",
    "            #save episode reward\n",
    "            print(i_episode_reward)\n",
    "            episode_reward[i_episode]=i_episode_reward\n",
    "            i_episode_reward=0\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_reward[:i_episode])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

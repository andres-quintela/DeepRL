{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DQN PyTorch Implemetation\n",
    "=====================================\n",
    "**Author**: `Andres Quintela`\n",
    "\n",
    "Based on the PyTorch DQN tutorial\n",
    "\n",
    "Arcade Learning Environment on Cartpole\n",
    "\n",
    "November 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "env = gym.make('Acrobot-v1').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    #saving a transition tuple\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    #sample a random number according to batch size\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "-------------\n",
    "\n",
    "3 convolutional layers and final linear fully conected layer.  \n",
    "Batch normalization is applied after every layer to eliminate outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.head = nn.Linear(784, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image extraction\n",
    "-------------\n",
    "\n",
    "The code below are utilities for extracting and processing rendered\n",
    "images from the environment. It uses the ``torchvision`` package, which\n",
    "makes it easy to compose image transforms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))  # transpose into torch order (CH\n",
    "\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TARGET_UPDATE = 5\n",
    "LEARNING_RATE=0.01\n",
    "\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.SGD(policy_net.parameters(),lr=LEARNING_RATE)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(3)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Loop\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing episode:  0\n",
      "-461.82317715883255\n",
      "Executing episode:  1\n",
      "-389.9931442029774\n",
      "Executing episode:  2\n",
      "-482.0519555211067\n",
      "Executing episode:  3\n",
      "-441.1384830661118\n",
      "Executing episode:  4\n",
      "-477.4401391148567\n",
      "Executing episode:  5\n",
      "-472.46340984106064\n",
      "Executing episode:  6\n",
      "-482.4965322613716\n",
      "Executing episode:  7\n",
      "-458.85182493925095\n",
      "Executing episode:  8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a0be61da7f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m#save episode reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-64a3ff469fc4>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# columns of actions taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Compute V(s_{t+1}) for all next states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9397f1db5980>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "max_t=500  #maximum timesteps per episode\n",
    "\n",
    "episode_reward=[0]*num_episodes\n",
    "i_episode_reward=0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    print('Executing episode: ',i_episode)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        state_numeric, reward, done, _ = env.step(action.item())\n",
    "        if (state_numeric[0]< 0):\n",
    "            reward = torch.tensor([reward+1-state_numeric[0]], device=device)\n",
    "        elif (state_numeric[0]>0):\n",
    "            reward = torch.tensor([reward+(1-state_numeric[0])], device=device)\n",
    "        else:\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        \n",
    "        #accumulated reward for each episode\n",
    "        i_episode_reward += reward.item()\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done or (t>max_t):\n",
    "            #save episode reward\n",
    "            print(i_episode_reward)\n",
    "            episode_reward[i_episode]=i_episode_reward\n",
    "            i_episode_reward=0\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(episode_reward[:i_episode])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

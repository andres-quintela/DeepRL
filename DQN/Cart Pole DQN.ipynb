{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DQN PyTorch Implemetation\n",
    "=====================================\n",
    "**Author**: `Andres Quintela`\n",
    "\n",
    "Based on the PyTorch DQN tutorial\n",
    "\n",
    "Arcade Learning Environment on Cartpole\n",
    "\n",
    "November 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    #saving a transition tuple\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    #sample a random number according to batch size\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "-------------\n",
    "\n",
    "2 convolutional layers and final linear fully conected layer.  \n",
    "Batch normalization is applied after every layer to eliminate outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.head = nn.Linear(1344, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image extraction\n",
    "-------------\n",
    "\n",
    "The code below are utilities for extracting and processing rendered\n",
    "images from the environment. It uses the ``torchvision`` package, which\n",
    "makes it easy to compose image transforms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEICAYAAACZJtWMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlqQG/H8lLkmp+KRmWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))  # transpose into torch order (CH\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "LEARNING_RATE=0.01\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.SGD(policy_net.parameters(),lr=LEARNING_RATE)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Loop\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0\n",
      "20.0\n",
      "19.0\n",
      "14.0\n",
      "17.0\n",
      "11.0\n",
      "15.0\n",
      "26.0\n",
      "42.0\n",
      "17.0\n",
      "28.0\n",
      "18.0\n",
      "13.0\n",
      "10.0\n",
      "16.0\n",
      "14.0\n",
      "18.0\n",
      "30.0\n",
      "38.0\n",
      "38.0\n",
      "17.0\n",
      "19.0\n",
      "20.0\n",
      "14.0\n",
      "38.0\n",
      "18.0\n",
      "24.0\n",
      "33.0\n",
      "14.0\n",
      "20.0\n",
      "26.0\n",
      "27.0\n",
      "23.0\n",
      "16.0\n",
      "57.0\n",
      "15.0\n",
      "28.0\n",
      "35.0\n",
      "25.0\n",
      "19.0\n",
      "30.0\n",
      "13.0\n",
      "17.0\n",
      "26.0\n",
      "22.0\n",
      "23.0\n",
      "39.0\n",
      "47.0\n",
      "39.0\n",
      "21.0\n",
      "29.0\n",
      "25.0\n",
      "18.0\n",
      "47.0\n",
      "11.0\n",
      "24.0\n",
      "23.0\n",
      "17.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "\n",
    "episode_reward=[0]*num_episodes\n",
    "i_episode_reward=0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        #accumulated reward for each episode\n",
    "        i_episode_reward += reward.item()\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done or (i_episode_reward >=200):\n",
    "            #save episode reward\n",
    "            print(i_episode_reward)\n",
    "            episode_reward[i_episode]=i_episode_reward\n",
    "            i_episode_reward=0\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(episode_reward[:i_episode])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Implementation on Gym's Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "import time as time\n",
    "\n",
    "from PIL import Image\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random as random\n",
    "import math as math\n",
    "%matplotlib inline\n",
    "\n",
    "#choose device cpu or cuda if a gpu is available\n",
    "device=torch.device('cpu')\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "# set up matplotlib to visulise images\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replay memory fucntion \n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):  #saves a transition\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define network class: 3 hidden layers with 8 neurons per layer\n",
    "\n",
    "\n",
    "#critic network input is the a state-action vector\n",
    "\n",
    "class DQN_critic(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size):\n",
    "        super(DQN_critic, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(input_size,8)\n",
    "        self.hidden2 = nn.Linear(8,8)\n",
    "        self.hidden3 = nn.Linear(8,8)\n",
    "        self.output = nn.Linear(8,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        return self.output(x.view(x.size(0), -1))\n",
    "    \n",
    "#actor network. Output is scaled in the action range and input is the state vector\n",
    "\n",
    "HIGH_BOUND=2\n",
    "LOW_BOUND=-2\n",
    "\n",
    "class DQN_actor(nn.Module):\n",
    "\n",
    "    def __init__(self,STATE_SIZE):\n",
    "        super(DQN_actor, self).__init__()\n",
    "        self.hidden1 = nn.Linear(STATE_SIZE,8)\n",
    "        self.hidden2 = nn.Linear(8,8)\n",
    "        self.hidden3 = nn.Linear(8,8)\n",
    "        self.output = nn.Linear(8,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        x=(torch.sigmoid(x)*(HIGH_BOUND - LOW_BOUND))+LOW_BOUND\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize neural nets\n",
    "#size of state vector\n",
    "STATE_SIZE=3\n",
    "\n",
    "#size of action vector, it is single-valued because actions are continuous in the interval (-2,2)\n",
    "ACTION_SIZE=1\n",
    "\n",
    "#critic net with input (s,a) tensor and output a single q value for that state-action pair\n",
    "critic_nn=DQN_critic(STATE_SIZE+ACTION_SIZE).to(device)\n",
    "target_critic_nn=DQN_critic(STATE_SIZE+ACTION_SIZE).to(device)\n",
    "\n",
    "#actor net: state input -- action output bounded from lower bound to high bound\n",
    "actor_nn=DQN_actor(STATE_SIZE).to(device)\n",
    "target_actor_nn=DQN_actor(STATE_SIZE).to(device)\n",
    "\n",
    "#initialize replay memory\n",
    "MEMORY_CAPACITY=1000000\n",
    "memory=ReplayMemory(MEMORY_CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#soft target update function\n",
    "def update_targets(target, original):\n",
    "        \"\"\"Weighted average update of the target network and original network\n",
    "            Inputs: target actor(critic) and original actor(critic)\"\"\"\n",
    "        \n",
    "        for targetParam, orgParam in zip(target.parameters(), original.parameters()):\n",
    "            targetParam.data.copy_((1 - TAU)*targetParam.data + TAU*orgParam.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model optimization by mini batch\n",
    "\n",
    "BATCH_SIZE=128\n",
    "GAMMA=0.99\n",
    "LEARNING_RATE_CRITIC=0.001\n",
    "LEARNING_RATE_ACTOR=0.001\n",
    "TAU=0.001\n",
    "\n",
    "target_critic_nn.load_state_dict(critic_nn.state_dict())\n",
    "optimizer_critic = optim.Adam(critic_nn.parameters(),lr=LEARNING_RATE_CRITIC)\n",
    "target_critic_nn.eval()\n",
    "\n",
    "\n",
    "target_actor_nn.load_state_dict(actor_nn.state_dict())\n",
    "optimizer_actor = optim.Adam(actor_nn.parameters(), lr=LEARNING_RATE_ACTOR)\n",
    "target_actor_nn.eval()\n",
    "\n",
    "def optimize_model():\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    #compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_mask_float = non_final_mask.type(torch.float).view(BATCH_SIZE, 1)\n",
    "    \n",
    "    #divide memory into different tensors\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action).view(BATCH_SIZE, -1)\n",
    "    reward_batch = torch.cat(batch.reward).view(BATCH_SIZE, 1)\n",
    "    \n",
    "    #create state-action (s,a) tensor for input into the critic network with taken actions\n",
    "    state_action=torch.cat([state_batch,action_batch], -1)\n",
    "    \n",
    "    #compute Q(s,a) using critic network\n",
    "    state_action_values = critic_nn(state_action)\n",
    "    \n",
    "    #compute deterministic next state action using actor target network \n",
    "    next_action = target_actor_nn(non_final_next_states).detach()\n",
    "    \n",
    "    #compute next timestep state-action (s,a) tensor for non-final next states\n",
    "    next_state_action = torch.zeros(BATCH_SIZE, 4, device=device)\n",
    "    next_state_action[non_final_mask,:] = torch.cat([non_final_next_states,next_action],-1)\n",
    "    \n",
    "    #compute next state values at t+1 using target critic network\n",
    "    next_state_values=target_critic_nn(next_state_action).detach()\n",
    "    \n",
    "    #compute expected state action values y[i]= r[i] + Q'(s[i+1],a[i+1])\n",
    "    expected_state_action_values = reward_batch + non_final_mask_float*GAMMA*next_state_values\n",
    "    \n",
    "    #critic loss by mean squared error\n",
    "    loss_critic=F.mse_loss(state_action_values,expected_state_action_values)\n",
    "    #optimize the critic network\n",
    "    optimizer_critic.zero_grad()\n",
    "    loss_critic.backward()\n",
    "    \n",
    "    for param in critic_nn.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "    optimizer_critic.step()\n",
    "    \n",
    "    #optimize actor\n",
    "    #actor actions\n",
    "    state_actor_action=actor_nn(state_batch)\n",
    "    #state-actor-actions tensor\n",
    "    state_actor_action_values=torch.cat([state_batch,state_actor_action],-1)\n",
    "    #loss\n",
    "    loss_actor=-1*torch.mean(critic_nn(state_actor_action_values))\n",
    "    optimizer_actor.zero_grad()\n",
    "    loss_actor.backward()\n",
    "    for param in actor_nn.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer_actor.step()\n",
    "                             \n",
    "    #soft parameter update\n",
    "    update_targets(target_critic_nn,critic_nn)\n",
    "    update_targets(target_actor_nn,actor_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#noise parameters\n",
    "\n",
    "# scale of the exploration noise process (1.0 is the range of each action\n",
    "# dimension)\n",
    "NOISE_SCALE_INIT = 0.1\n",
    "\n",
    "# decay rate (per episode) of the scale of the exploration noise process\n",
    "NOISE_DECAY = 0.99\n",
    "\n",
    "# parameters for the exploration noise process:\n",
    "# dXt = theta*(mu-Xt)*dt + sigma*dWt\n",
    "EXPLO_MU = 0.0\n",
    "EXPLO_THETA = 0.15\n",
    "EXPLO_SIGMA = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_steps=1000\n",
    "episodes=100\n",
    "episode_reward=[0]*episodes\n",
    "EPS=0.001\n",
    "\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    \n",
    "    state = env.reset() #initial state\n",
    "    state = torch.tensor([state],dtype=torch.float)\n",
    "    \n",
    "    # Initialize exploration noise process, parameters in parameters file\n",
    "    noise_process = np.zeros(ACTION_SIZE)\n",
    "    noise_scale = (NOISE_SCALE_INIT * NOISE_DECAY**EPS) * (HIGH_BOUND - LOW_BOUND)\n",
    "    done = False\n",
    "    for t in count():\n",
    "        \n",
    "        action=actor_nn(state).detach() #deterministic choice of a using actor network\n",
    "        # add temporally-correlated exploration noise to action (using an Ornstein-Uhlenbeck process)\n",
    "        noise_process = EXPLO_THETA * (EXPLO_MU - noise_process) + EXPLO_SIGMA * np.random.randn(ACTION_SIZE)\n",
    "        noise=noise_scale*noise_process\n",
    "        action += torch.tensor([noise[0]],dtype=torch.float,device=device)\n",
    "        #perform an action\n",
    "        next_state,reward,done,_=env.step(action)\n",
    "        next_state=torch.tensor([next_state],dtype=torch.float, device=device)\n",
    "        reward = torch.tensor([reward],device=device,dtype=torch.float, device=device)\n",
    "\n",
    "        episode_reward[i_episode] += reward.item()\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        #save transition into memory\n",
    "        memory.push(state,action,next_state,reward)\n",
    "\n",
    "        #move to the next state\n",
    "        state=next_state\n",
    "\n",
    "        #optimize de model\n",
    "        optimize_model()\n",
    "        #show the image\n",
    "        \n",
    "        #env.render()\n",
    "        if t>max_steps or done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(episode_reward[:i_episode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
